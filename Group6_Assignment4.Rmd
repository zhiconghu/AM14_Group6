---
title: "Group6_Assignment4"
author: "Zhicong Hu"
date: "23/02/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r loadlibraries, include = FALSE , message=FALSE, warning=FALSE}
library(here)
library(readxl)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(lubridate)
library(moments)
library(kableExtra)
```

# Assignment 4: Measuring Financial Risk and Big Data

## AM14 Empirical Finance - Study Group 6

## Introduction

## Q1: Implementing Value at Risk (VaR) in R

**Pick three stocks from the daily stock returns data set (PS1 Daily.xlsx) and transform these simple returns to log returns.**

```{r message=FALSE, warning=FALSE}
data <- read_excel(here("Data", "PS1_Daily.xlsx"), sheet = "HPR_daily")
colnames(data) <- data[1,]
data <- data[-1,]

# Convert date column to date variable
data$DATE <- ymd(data$DATE)

# Convert all other columns to numeric 
data <- data %>% mutate_if(is.character, as.numeric)

# Pick MSFT, INTC, JPM
data <- data %>% select(DATE, MSFT, INTC, JPM)

# Transform to log returns
data[c("MSFT","INTC","JPM")] <- lapply(data[c("MSFT","INTC","JPM")], function(vector) log(1+vector)) %>% as.data.frame()
```

**Estimate three volatility time series for each of these three stocks by either using a MA (10 weeks) or an EWMA (**$\lambda = 0.94$ and $\sigma_{0}^2=\frac{1}{T}\sum^{T}_{t=1}\sigma_{t}^2$, where T is number of observations of daily returns in your sample) model.

```{r}
# Parameters for MA
window_length <- 50

# Initiate columns for MA
data$MSFT_MA <- NA
data$INTC_MA <- NA
data$JPM_MA <- NA

# For loop to calculate EWMA
for (i in (window_length+1):nrow(data)) {
  data[i, "MSFT_MA"] <- sum(data[(i-window_length):(i-1), "MSFT"]^2)/window_length
  data[i, "INTC_MA"] <- sum(data[(i-window_length):(i-1), "INTC"]^2)/window_length
  data[i, "JPM_MA"] <- sum(data[(i-window_length):(i-1), "JPM"]^2)/window_length
}

# Parameters for EWMA
lambda <- 0.94
initial_EWMA <- c(sum(data$MSFT^2)/length(data$MSFT),
                  sum(data$INTC^2)/length(data$INTC),
                  sum(data$JPM^2)/length(data$JPM))

# Initiate columns for EWMA
data$MSFT_EWMA <- c(initial_EWMA[1], rep(NA, nrow(data)-1))
data$INTC_EWMA <- c(initial_EWMA[2], rep(NA, nrow(data)-1))
data$JPM_EWMA <- c(initial_EWMA[3], rep(NA, nrow(data)-1))

# For loop to calculate EWMA
for (i in 2:nrow(data)) {
  data[i, "MSFT_EWMA"] <- (1-lambda)*(data[(i-1), "MSFT"])^2 + lambda*(data[(i-1), "MSFT_EWMA"])
  data[i, "INTC_EWMA"] <- (1-lambda)*(data[(i-1), "INTC"])^2 + lambda*(data[(i-1), "INTC_EWMA"])
  data[i, "JPM_EWMA"] <- (1-lambda)*(data[(i-1), "JPM"])^2 + lambda*(data[(i-1), "JPM_EWMA"])
}
```

**Based on these six time series (two volatility time series for each of the three stocks) calculate the daily one day Value-at-Risk (VaR) 95% assuming normality. That is, you should use the estimated volatility time series together with the following formula for conditional VaR assuming normality**

$$VaR_{95\%,t} = \bar{r} - \Phi^{-1}(0.05) \times \sigma_{t}$$

**where** $\bar{r}$ is the mean return (i.e., the average return of the return series of interest up to time $t$), $\Phi^{-1}$ is the inverse of the standard normal cumulative density function and, hence, $\Phi^{-1}(0.05) = 1.65$ (the z score!). Moreover, $\sigma_{t}$ is your estimated volatility at time $t$.

```{r}
# Parameters for VaR calculations
phi <- 1.65

# Initiate columns for MA VaR
data$MSFT_MA_VaR <- NA
data$INTC_MA_VaR <- NA
data$JPM_MA_VaR <- NA

# For loop to calculate MA VaR
for (i in (window_length+1):nrow(data)) {
  data[i, "MSFT_MA_VaR"] <- sum(data[1:(i-1), "MSFT"])/i - sqrt(phi*data[i, "MSFT_MA"])
  data[i, "INTC_MA_VaR"] <- sum(data[1:(i-1), "INTC"])/i - sqrt(phi*data[i, "INTC_MA"])
  data[i, "JPM_MA_VaR"] <- sum(data[1:(i-1), "JPM"])/i - sqrt(phi*data[i, "JPM_MA"])
}

# Initiate columns for EWMA VaR
data$MSFT_EWMA_VaR <- NA
data$INTC_EWMA_VaR <- NA
data$JPM_EWMA_VaR <- NA

# For loop to calculate EWMA VaR
for (i in 2:nrow(data)) {
  data[i, "MSFT_EWMA_VaR"] <- sum(data[1:(i-1), "MSFT"])/i - sqrt(phi*data[i, "MSFT_EWMA"])
  data[i, "INTC_EWMA_VaR"] <- sum(data[1:(i-1), "INTC"])/i - sqrt(phi*data[i, "INTC_EWMA"])
  data[i, "JPM_EWMA_VaR"] <- sum(data[1:(i-1), "JPM"])/i - sqrt(phi*data[i, "JPM_EWMA"])
}

# For the first day
data[1, "MSFT_EWMA_VaR"] <- data[1, "MSFT"] - sqrt(phi*data[1, "MSFT_EWMA"])
data[1, "INTC_EWMA_VaR"] <- data[1, "INTC"] - sqrt(phi*data[1, "INTC_EWMA"])
data[1, "JPM_EWMA_VaR"] <- data[1, "JPM"] - sqrt(phi*data[1, "JPM_EWMA"])
```

**In a last step, you are supposed to "backtest" your VaR estimates. That is, for each stock you now have three VaR series as well as the realized returns. With this data, count for each VaR estimate separately the number of violations. In other words, count the negative realized market returns that are more extreme than the VaR on this given day. For example, a violation of VaR occurs on a day when the realized returns is -9% and the VaR is -8%. How many violations would you expect if your VaR estimates were to be accurate (i.e., true)? How many violations do you observe? What do you conclude?**

```{r}
# Mutate violation column
data <- data %>% 
  mutate(MSFT_MA_VaR_violations = ifelse(MSFT < MSFT_MA_VaR, 1, 0),
         INTC_MA_VaR_violations = ifelse(INTC < INTC_MA_VaR, 1, 0),
         JPM_MA_VaR_violations = ifelse(JPM < JPM_MA_VaR, 1, 0),
         MSFT_EWMA_VaR_violations = ifelse(MSFT < MSFT_EWMA_VaR, 1, 0),
         INTC_EWMA_VaR_violations = ifelse(INTC < INTC_EWMA_VaR, 1, 0),
         JPM_EWMA_VaR_violations = ifelse(JPM < JPM_EWMA_VaR, 1, 0),)
```

If our log return's are normally distributed, we will assume that the number of violations is around 5% of the total trading days. This is because we used the Z-score of 1.65 in our VaR calculation, which is the Z-score of the 90% confidence interval. This means that the probability of performing below the VaR at any given day should be 5%.

```{r}
violations <- colSums(data %>% select(MSFT_MA_VaR_violations, INTC_MA_VaR_violations, JPM_MA_VaR_violations,
                                      MSFT_EWMA_VaR_violations, INTC_EWMA_VaR_violations, JPM_EWMA_VaR_violations), na.rm = TRUE) 
violations
```

```{r message=FALSE, warning=FALSE}
data.frame(t(violations)) %>% 
  pivot_longer(cols = everything(), names_to = "Type", values_to = "Number of Violations") %>% 
  separate(Type, c("Stock", "Type")) %>% 
  mutate(`Total Days` = ifelse(Type == "MA", nrow(data)-50, nrow(data))) %>% 
  mutate(Percent = round((`Number of Violations`/`Total Days`)*100, 2)) %>% 
  select(-`Number of Violations`, -`Total Days`) %>% 
  pivot_wider(names_from = "Type", values_from = "Percent")
```

We can see that the percentage of violation is much higher than 5% for every stock, for both method of calculating volatility, 10 Week MA and EWMA. This suggest that the log returns of stocks are not normally distribution but instead has very heavy fat tails. Therefore, our assumption of normally distribution log returns and using Z-score for calculating VaR is invalid and results in higher than expected for number of violations.

## Q2: Portfolio Data Loading

**Go to Kenneth French's webpage <https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data> library.html and download the 10 portfolios formed on operating profitability, investment, dividend yield, and momentum and the 49 industry portfolios at the monthly frequency. In addition, also download the Fama/French 3 factors at the monthly frequency.**

```{r message=FALSE, warning=FALSE}
OP_portfolios <- read_csv(here("Data", "Portfolios_Formed_on_OP.CSV"), skip = 24) %>% 
  rename(Date = `...1`) # change to X1 if there is an error (same for the following data)
OP_portfolios <- OP_portfolios[1:(which(is.na(as.numeric(OP_portfolios$Date)))[1]-1),]

I_portfolios <- read_csv(here("Data", "Portfolios_Formed_on_INV.CSV"), skip = 17) %>% 
  rename(Date = `...1`)
I_portfolios <- I_portfolios[1:(which(is.na(as.numeric(I_portfolios$Date)))[1]-1),]

DY_portfolios <- read_csv(here("Data", "Portfolios_Formed_on_D-P.CSV"), skip = 19) %>% 
  rename(Date = `...1`)
DY_portfolios <- DY_portfolios[1:(which(is.na(as.numeric(DY_portfolios$Date)))[1]-1),]

M_portfolios <- read_csv(here("Data", "10_Portfolios_Prior_12_2.CSV"), skip = 10) %>% 
  rename(Date = `...1`)
M_portfolios <- M_portfolios[1:(which(is.na(as.numeric(M_portfolios$Date)))[1]-1),]

I49_portfolios <- read_csv(here("Data", "49_Industry_Portfolios.CSV"), skip = 11) %>% 
  rename(Date = `...1`)
I49_portfolios <- I49_portfolios[1:(which(is.na(as.numeric(I49_portfolios$Date)))[1]-1),]

factors <- read_csv(here("Data", "F-F_Research_Data_Factors.CSV"), skip = 3) %>% 
  rename(Date = `...1`)
factors <- factors[1:(which(is.na(as.numeric(factors$Date)))[1]-1),]

# Set Data column
OP_portfolios$Date <- as.Date(paste0(OP_portfolios$Date, "01"), format = "%Y%m%d")
I_portfolios$Date <- as.Date(paste0(I_portfolios$Date, "01"), format = "%Y%m%d")
DY_portfolios$Date <- as.Date(paste0(DY_portfolios$Date, "01"), format = "%Y%m%d")
M_portfolios$Date <- as.Date(paste0(M_portfolios$Date, "01"), format = "%Y%m%d")
I49_portfolios$Date <- as.Date(paste0(I49_portfolios$Date, "01"), format = "%Y%m%d")
factors$Date <- as.Date(paste0(factors$Date, "01"), format = "%Y%m%d")

# Set numeric
OP_portfolios <- OP_portfolios %>% mutate_if(is.character, as.numeric)
I_portfolios <- I_portfolios %>% mutate_if(is.character, as.numeric)
DY_portfolios <- DY_portfolios %>% mutate_if(is.character, as.numeric)
M_portfolios <- M_portfolios %>% mutate_if(is.character, as.numeric)
I49_portfolios <- I49_portfolios %>% mutate_if(is.character, as.numeric)
factors <- factors %>% mutate_if(is.character, as.numeric)

# Clean I49_portfolios
I49_portfolios$Soda[I49_portfolios$Soda < -99] <- NA
I49_portfolios$Hlth[I49_portfolios$Hlth < -99] <- NA
I49_portfolios$Rubbr[I49_portfolios$Rubbr < -99] <- NA
I49_portfolios$FabPr[I49_portfolios$FabPr < -99] <- NA
I49_portfolios$Guns[I49_portfolios$Guns < -99] <- NA
I49_portfolios$Gold[I49_portfolios$Gold < -99] <- NA
I49_portfolios$PerSv[I49_portfolios$PerSv < -99] <- NA
I49_portfolios$Softw[I49_portfolios$Softw < -99] <- NA
I49_portfolios$Paper[I49_portfolios$Paper < -99] <- NA
```

## Q3: Principal Component Analysis

**Run a principal component on the combined excess returns of the 10 portfolios formed on operating profitability, investment, dividend yield, and momentum and the 49 industry portfolios (hint: use the prcomp command discussed in lecture 5). How many components/factors are needed to explain 95% of the return variation?**

```{r pca, message = FALSE, warning=FALSE}
# joining data
all_portfolios <- OP_portfolios %>% 
  inner_join(I_portfolios, by = "Date", suffix = c("_OP", "_I")) %>% 
  inner_join(DY_portfolios, by = "Date", suffix = c("", "_DY")) %>% 
  inner_join(M_portfolios, by = "Date", suffix = c("", "_M")) %>% 
  inner_join(I49_portfolios, by = "Date", suffix = c("", "_I49")) %>% 
  inner_join(factors %>% select(Date, risk_free = RF), by = "Date") %>% 
  mutate(across(!risk_free & !Date, ~ . -risk_free)) %>% 
  select(-risk_free) %>% 
  drop_na()

# creating pca object with prcomp()
pca <- prcomp(all_portfolios %>% select(-Date), scale = T, center = T)

# summary of results
summary(pca)
```

From the cumulative proportion of our principal components in the summary results above, we can see that we need 36 components to explain 95% of the return variations.

## Q4: Regression on Factors

Run the following regressions for the 10 portfolios formed on operating profitability, investment, dividend yield, and momentum and the 49 industry portfolios and save all regression adjusted $R^2$:

$$r_{i,t} − r_{f,t} = \alpha_i + \beta_i(r_{MKT,t} − r_{f,t}) + \gamma_ir_{SMB,t} + \delta_ir_{HML,t} + \epsilon_{i,t}$$

What is the average and median regression adjusted $R^2$? What is the standard deviation of adjusted $R^2$?

```{r regresion on fama-french factors, message = FALSE, warning=FALSE}
# create empty list for results
factors_r2_vector <- c()
# regression
for (i in 2:(length(all_portfolios))){
  temp_df <- all_portfolios[,c(1,i)] %>% inner_join(factors %>% select(-RF), by = "Date") %>% select(-Date)
  temp_model <- lm(temp_df)
  factors_r2_vector <- append(factors_r2_vector, summary(temp_model)$adj.r.squared)
}
# printing results
cat("The average adjusted R-squared is", mean(factors_r2_vector),
    "\nThe median adjusted R-squared is", median(factors_r2_vector),
    "\nThe standard deviation of adjusted R-squared is", sd(factors_r2_vector))
```

The average adjusted R-squared is 0.7573 and median adjusted R-squared is 0.8272. The standard deviation of R-squared is 0.1875.

## Q5: Regression on Principal Components

Now, run the following regressions for the 10 portfolios formed on operating profitability, investment, dividend yield, and momentum and the 49 industry portfolios and save all regression adjusted $R^2$:

$$r_{i,t} − r_{f,t} = \alpha_i + \beta_ir_{PC1,t} + \gamma_ir_{PC2,t} + \delta_ir_{PC3,t} + \epsilon_{i,t}$$

(hint: $r_{PC1,t}$, $r_{PC2,t}$, $r_{PC3,t}$ are simply the first three principal components. These components are returns themselves as the principal components of excess returns are simply linear combinations (i.e., portfolios) thereof, i.e., excess returns. Also note that you can call these first three principal components as follows in R: pca\$x[,1], pca\$x[,2], and pca\$x[,3].)

What is the average and median regression adjusted $R^2$? What is the standard deviation of adjusted $R^2$? Compare and discuss your results with the ones from above!

```{r regression on PC, message=FALSE, warning=FALSE}
# putting first 3 principle components into data frame
pc_df <- data.frame(Date = all_portfolios$Date, PC1 = pca$x[,1], PC2 = pca$x[,2], PC3 = pca$x[,3])

# creating empty list for results
pc_r2_vector <- c()
# regression
for (i in 2:(length(all_portfolios))){
  temp_df <- all_portfolios[,c(1,i)] %>% inner_join(pc_df, by = "Date") %>% select(-Date)
  temp_model <- lm(temp_df)
  pc_r2_vector <- append(pc_r2_vector, summary(temp_model)$adj.r.squared)
}
# printing results
cat("The average adjusted R-squared is", mean(pc_r2_vector),
    "\nThe median adjusted R-squared is", median(pc_r2_vector),
    "\nThe standard deviation of adjusted R-squared is", sd(pc_r2_vector))
```

Using first three principle components as factors, the average adjusted R-squared is 0.7907 and median adjusted R-squared is 0.8662, which are both higher than using Fama-French factors. The standard deviation of R-squared is lower than using Fama-French factors. This means that principle components are better predictors for excess return since they account for more variation in excess return.

## Q6: Limitations of Principal Component Factors

Despite the better performance of the PC regression model above, principle components have limitations in that they are less intuitive and not interpretable since they don't have an exact definition, whereas the Fama-French factors can be understood easily. Another major limitation is that it is optimized based on our current data set, but may not perform as well in the future.
